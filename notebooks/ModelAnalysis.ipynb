{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0a8168dad468f0441bbb6e967433b38f91f1091584c45e1ceabd47b46bd3b23c4",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/federated/history/logdir/experiment_name\" # Absolute path to the experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from federated.data.data_preprocessing import load_data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that evaluates the model.\n",
    "Prints accuracy and loss value\n",
    "\"\"\"\n",
    "def evaluation(X, y, model):\n",
    "    scores = model.evaluate(X, y, verbose=0)\n",
    "    print(f\"Accuracy: {scores[1]*100}%, Loss: {scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"N\", \"S\", \"V\", \" F\", \"U\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that creates confusion matrix based upon the dataset and the model.\n",
    "Displays the confusion matrix.\n",
    "\"\"\"\n",
    "\n",
    "def make_confusion_matrix(X,y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    y_test = np.argmax(y, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    cnf_matrix =  cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(LABELS))\n",
    "    plt.xticks(ticks, LABELS, rotation=45)\n",
    "    plt.yticks(ticks, LABELS)\n",
    "\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j,i,format(cnf_matrix[i,j], '.2f'), horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cnf_matrix[i, j] > cnf_matrix.max()/2 else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "    plt.savefig(f\"{PATH}/images/confusion_matrix.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that loads model and dataset for analysis\n",
    "Returns x_test, y_test and model\n",
    "\"\"\"\n",
    "def load(name):\n",
    "    model = tf.keras.models.load_model(PATH)\n",
    "    X_test, y_test = load_data(data_analysis=True)\n",
    "\n",
    "    X_test = X_test.reshape(len(X_test), X_test.shape[1],1)\n",
    "    evaluation(X_test, y_test, model)\n",
    "    \n",
    "    return X_test, y_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_event(path, type, moments=False):\n",
    "\n",
    "    path += f\"/{type}/\"\n",
    "    event_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    event_files = list(filter(lambda f: \"empty\" not in f, event_files))\n",
    "\n",
    "    event_files_ids = [int(event.split(\".\")[-2]) for event in event_files]\n",
    "    index = event_files_ids.index(max(event_files_ids))\n",
    "    \n",
    "    path += event_files[index]\n",
    "\n",
    "    if moments:\n",
    "        condition = lambda x : \"privacy_loss\" in x\n",
    "    else:\n",
    "        condition = lambda x : \"loss\" in x or \"accuracy\" in  x\n",
    "\n",
    "    metrics = defaultdict(list)\n",
    "    for e in summary_iterator(path):\n",
    "        for v in e.summary.value:\n",
    "            if isinstance(v.simple_value, float) and condition(v.tag):\n",
    "                if v.simple_value == 0.0:\n",
    "                    metrics[f\"{v.tag}_{type}\"].append(tf.make_ndarray(v.tensor))\n",
    "                else:\n",
    "                    metrics[f\"{v.tag}_{type}\"].append(v.simple_value)\n",
    "\n",
    "    metrics_df = pd.DataFrame({k: v for k,v in metrics.items()})\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(type, moments=False):\n",
    "    \n",
    "    if type not in [\"accuracy\", \"loss\", \"moments_accountant\"]:\n",
    "        raise ValueError(f\"type must be accuracy or loss, not {type}\")\n",
    "\n",
    "    path = PATH\n",
    "    if moments:\n",
    "        moments_df = dataframe_from_event(path, type)\n",
    "        moments_df= moments_df.rename(columns={\"cumulative_privacy_loss_moments_accountant\": \"cumulative_privacy_loss\"})\n",
    "        return moments_df\n",
    "    else:\n",
    "        train_df = dataframe_from_event(path, \"train\")\n",
    "        validation_df = dataframe_from_event(path, \"validation\")\n",
    "\n",
    "        cols = list(train_df.columns) + list(validation_df.columns)\n",
    "\n",
    "        return pd.concat([train_df, validation_df], axis=1)[[col for col in cols if type in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, model_centralized = load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_centralized.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, y_pred = make_confusion_matrix(X_test, y_test, model_centralized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = plot_graph(\"accuracy\")\n",
    "metrics_df = metrics_df.copy()\n",
    "\n",
    "for col in metrics_df.columns:\n",
    "    if \"validation\" in col: \n",
    "        metrics_df.rename(columns={col: 'Validation Accuracy'}, inplace=True)\n",
    "    else:\n",
    "        metrics_df.rename(columns={col: 'Training Accuracy'}, inplace=True)\n",
    "\n",
    "fig = metrics_df.plot(labels=dict(index=\"Epoch\", value=\"Accuracy\"))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(f\"{PATH}/images/accuracy.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = plot_graph(\"loss\")\n",
    "metrics_df = metrics_df.copy()\n",
    "\n",
    "for col in metrics_df.columns:\n",
    "    if \"validation\" in col: \n",
    "        metrics_df.rename(columns={col: 'Validation Loss'}, inplace=True)\n",
    "    else:\n",
    "        metrics_df.rename(columns={col: 'Training Loss'}, inplace=True)\n",
    "\n",
    "metrics_df.index += 1\n",
    "\n",
    "fig = metrics_df.plot(labels=dict(index=\"Epoch\", value=\"Loss\", variable=\"\"))\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(f\"{PATH}/images/loss.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    metrics_df = plot_graph(\"moments_accountant\", moments=True)\n",
    "    metrics_df = metrics_df.copy()\n",
    "\n",
    "    metrics_df.rename(columns={\"cumulative_privacy_loss\": \"Privacy Loss\"}, inplace=True)\n",
    "\n",
    "    fig = metrics_df.plot(labels=dict(index=\"Epoch\", value=\"Îµ\", variable=\"\"))\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_image(f\"{PATH}/images/moments_accountant.pdf\")\n",
    "except:\n",
    "    print(\"No moments accountant.\")"
   ]
  }
 ]
}